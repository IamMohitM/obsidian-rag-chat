{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-872492b0dbf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, StorageContext, load_index_from_storage, Document\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "model_name = os.environ.get('AZURE_OPENAI_MODEL')\n",
    "embed_model_name = 'text-embedding-3-small'\n",
    "deployment_name = os.environ.get('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=model_name,\n",
    "    deployment_name=deployment_name,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=embed_model_name,\n",
    "    deployment_name=embed_model_name,\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.readers.obsidian import ObsidianReader\n",
    "\n",
    "from typing import Any, List\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import MarkdownReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_extractor(filepath):\n",
    "    return {\"filepath\": filepath, 'filename': os.path.basename(filepath)}\n",
    "\n",
    "class ObsidianReaderWithMetaData(ObsidianReader):\n",
    "    \"\"\"Utilities for loading data from an Obsidian Vault.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the vault.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n",
    "        \"\"\"Load data from the input directory.\"\"\"\n",
    "        docs: List[Document] = []\n",
    "        metadata_extractor = load_kwargs.get(\"metadata_extractor\") if load_kwargs.get(\"metadata_extractor\") else None\n",
    "        for dirpath, dirnames, filenames in os.walk(self.input_dir):\n",
    "            dirnames[:] = [d for d in dirnames if not d.startswith(\".\")]\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".md\"):\n",
    "                    filepath = os.path.join(dirpath, filename)\n",
    "                    content = MarkdownReader().load_data(Path(filepath))\n",
    "                    for c in content:\n",
    "                        if metadata_extractor:\n",
    "                            c.metadata = metadata_extractor(filepath)\n",
    "                        c.doc_id = filepath\n",
    "                    docs.extend(content)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_notes = r'/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ObsidianReaderWithMetaData(\n",
    "    tech_notes,\n",
    ").load_data(metadata_extractor=metadata_extractor)\n",
    "index = VectorStoreIndex.from_documents(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', 'filename': 'OpenAI API.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='azure models of interest - \\n- text-embedding-3-small - **text-embedding-3-small**,\\xa0**1**\\n- **dall-e-3**,\\xa0**3.0**\\n\\n- Essentially, with Azure openai api, you need to first deploy the models and then you can use them\\n- dalle-3 is used for image generation and not gpt4o-mini or gptx models\\n- ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/C++.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/C++.md', 'filename': 'C++.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"All is well. C++ is a funny language because it's not an interpreted language and uses Object oriented programming context. It was written by Mohit Motwani.\\n\\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Yo Yo yo.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Yo Yo yo.md', 'filename': 'Yo Yo yo.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\nPython is a better language than c++ because it is a compiled language. It was written by Prerna Motwani', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', 'filename': 'AdamW.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\nAdamW updates the weight decay part of adam\\n- Weight decay is a regularisation applied to large weights in the NN to move them towards zero\\n\\t- low weight decay could cause over-fitting\\n\\t- high weight decay could cause under-fitting', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n- ^Character level language Modelling - The basic unit is a character ^a60c01\\n\\t- Therefore a much smaller vocabulary size\\n\\t- \\'The cat sat on the m\\' - the prediction would the character \\'a\\' to get to the word mat for this language model\\n\\t- Predicts on character level\\n\\t- Can handle out of vocabulary words and misspellings easily\\n- Word level language Modelling - The basic unit is a word\\n\\t- Much bigger vocabulary size\\n\\t- Output is much more coherent\\n\\n> Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively **interpolates between word level inputs for frequent symbol sequences and char- acter level inputs for infrequent symbol sequences.**\\n\\nClaude Explanation for Byte Pair\\n\\n\\n1. Character-level vs. Word-level Language Modeling:\\n\\nCharacter-level Language Modeling:\\n- Operates at the level of individual characters (letters, numbers, punctuation)\\n- Predicts the next character in a sequence\\n- Has a smaller vocabulary size (typically 26 letters, 10 digits, and some special characters)\\n- Can handle out-of-vocabulary words and misspellings more easily\\n\\nExample:\\nInput: \"The cat sat on the m\"\\nTask: Predict the next character (likely \"a\" to form \"mat\")\\n\\nWord-level Language Modeling:\\n- Operates at the level of whole words\\n- Predicts the next word in a sequence\\n- Has a larger vocabulary size (typically tens or hundreds of thousands of words)\\n- Generally produces more coherent output for larger contexts\\n\\nExample:\\nInput: \"The cat sat on the\"\\nTask: Predict the next word (e.g., \"mat\", \"chair\", \"roof\")\\n\\n2. Byte Pair Encoding (BPE):\\n\\nByte Pair Encoding, as explained in the GPT-2 paper \"Language Models are Unsupervised Multitask Learners,\" is a subword tokenization method that bridges the gap between character-level and word-level modeling.\\n\\nKey concepts:\\na) It starts with a vocabulary of individual characters and iteratively merges the most frequent pair of consecutive bytes or characters.\\nb) This process creates a vocabulary of subword units, which can represent both common words and parts of less common words.\\n\\nHow it works:\\n1. Initialize the vocabulary with individual characters.\\n2. Count the frequency of adjacent pairs of symbols in the training data.\\n3. Merge the most frequent pair and add it to the vocabulary.\\n4. Repeat steps 2-3 for a predetermined number of merges or until a desired vocabulary size is reached.\\n\\nExample:\\nLet\\'s say we have the following text: \"The dancer dances beautifully\"\\n\\nInitial vocabulary: [\\'T\\', \\'h\\', \\'e\\', \\' \\', \\'d\\', \\'a\\', \\'n\\', \\'c\\', \\'r\\', \\'s\\', \\'b\\', \\'u\\', \\'t\\', \\'i\\', \\'f\\', \\'l\\', \\'y\\']\\n\\nAfter some merges:\\n[\\'The\\', \\' \\', \\'d\\', \\'an\\', \\'c\\', \\'er\\', \\' \\', \\'d\\', \\'an\\', \\'ces\\', \\' \\', \\'b\\', \\'eau\\', \\'t\\', \\'i\\', \\'f\\', \\'ul\\', \\'ly\\']\\n\\nBenefits of BPE in the context of GPT-2:\\n1. Adaptability: It can handle both common and rare words effectively.\\n2. Efficiency: It creates a compact vocabulary that balances the trade-offs between character-level and word-level models.\\n3. Handling of unknown words: It can often break down unknown words into meaningful subword units.\\n\\nBy using BPE, GPT-2 achieves a balance between the flexibility of character-level models and the semantic coherence of word-level models. This allows it to work effectively across a wide range of languages and domains without requiring language-specific preprocessing or tokenization.\\n\\nWould you like me to elaborate on any specific aspect of these concepts?\\n\\n\\n\\n\\n\\n[[Metrics#Perplexity]]\\n\\nGPT 2 is trained on web text but evaluated on different datasets - children\\'s book test, lambada, etc without any fine tuning - Zero Shot Setting.\\n\\n\\nThe recommended way for creating train and test-splits for this dataset is to use \\n\\nZero shot performance could be used as baseline for performance for fine-tuning \\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nSummary\\n\\n- A webtext dataset is generated using reddit outbound links with at least 3 karma - this implies (implicitly) that the data is curated\\n\\t- About 40 gb data set size with 8 million documents\\n- The tokenization is [[#^a60c01]]byte pair encoding (BPE). It is a middle ground between word-level and character level encoding. It iteratively merges various bytes (creating byte pairs/tokens) based on their frequency evaluated from the dataset into tokens\\n\\t- From Kaparthy's video - this is entire separate module of LMs. The LMs never see the actual text, they only see the tokens. Very important part of LMs.\\n\\t- This type of encoding allows for working with non-frequent word pairs as well and working with different languages.\\n- Evaluated on different datasets with a zero shot setting - Used web text as the dataset for training, then tested on different dataset without fine tuning. \\n- Use bloom filter of 8-grams data structure to identify duplicates and to create train and test splits.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='- When checking the weight initialisation, check if the model is not favouring any particular class\\n\\t- The average expected loss could be computed when the model is initialised\\n\\t- For example, if we are using cross entropy loss and we have 50000 tokens, the token probabilities must be close to 1/50000 = 0.00002 and cross entropy loss should then be\\n\\t\\t-log(1/50000) = 10.8\\n\\t- Note above that cross entropy for single label will amount to - $-y_{gt}log(y_{pred})$  and $y_{gt}$ is $1$ and $y_{pred}$ is expected to be $1/50000$\\n- Overfit on a single batch\\n\\t- To check if your model is capable of learning, overfit on a single batch.\\n\\t- You must use only one batch, train your model for some epochs and then verify if the loss is decreasing over epochs (overfitting) to the batch\\n\\n- Transformer - Input and Output embeddings are the same\\n\\t- This is a type of inductive bias\\n\\t-  You want semantically similar words/tokens (lower-case\\\\==upper-case, synonyms etc) to have similar embeddings (at input) and probabilities (at output)\\n\\t- Also this makes the network very efficient for training and inference.\\n\\n- Weight initialisation\\n\\t- When using residual layers, we keep adding the activations to the input. We need to account for this during initialisation\\n\\t- As we keep adding inputs, the std keeps growing over the layers. We need to therefore scale the weight initialisation with respect to the number of residual layers in the network\\n\\n`torch.cuda.synchronize` - When we use torch, we actually scheduling a set of instructions on the GPU. When we use the above function, we essentially wait for the GPU to finish processing these set of instructions on the GPU and therefore synchronise the CPU and the GPU.\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='\\n\\nOptimizations\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\nUsing Tensor Cores - TF 32\\nUsing FP32 and TF32 - In TF32, the inputs and outputs are float 32. But **during the computation**, the 32 bit is reduced to 19 bits (the truncation happens in the mantissa bits \\\\[13 bits are dropped]) for computation. This is supported on certain NVIDIA GPUS like A100. Note that TF 32 is performed on tensor cores and only the computation is done in TF 32. The movement of data across different kernels and devices is still Float 32.\\n\\n\\n\\n\\nYou can set this in torch using `torch.set_float32_matmul_precision('high') #tf32`\\n\\n\\nBFloat16 - has the same range as TF 32 and float 32, but much more aggressive in dropping the precision/mantissa part of the floating point.\\n\\n---\\nThe range can be understood with an example - the range of a certain type could be from -100 to 100. The range is about 201 different numbers. If the precision of two types is different, then it comes down to how many decimal points for each number can be presented. for example type 1 could represent a number like 67.12311 and type 2 could only represent a number up to 67.1. \\n\\n> The above is just an example\\n\\n---\\n\\n\\nFp16 came first for Volta series but requires gradient scaling. BFloat 16  \\n\\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\ntorch.compile\\n\\n- Think of torch.compile as gcc for neural nets\\n\\nWhen you provide the model to torch.compile, sees your entire code at run time, sees the operations that you want to run and optimize process. \\n- Remove the python interpreter from the forward pass entirely (this also means that you won't be able to debug while using this)\\n- It will compile the NN into a single object\\n- GPU read/writes are optimised.  One way is kernel fusion. Obviously you need to know what you are going to compute before hand for kernel fusion.\\n\\n\\t> Kernel fusion is essentially combining multiple operations into 1 or fewer operations. This ensures that loading input and output from memory to cores is significantly decreases as only one kernel operations need to be performed instead of multiple.\\n\\t> \\n\\t> The intermediate calculations reside on the chip in SRAM (which is usually very small). \\n\\t> \\n\\t> If multiple kernels are used, the input is loaded to cores and then returned back to memory and then this taken by the next kernel and so on. This travelling back and forth between memory (GPU read/write) and GPU cores is expensive.\\n\\n-  There are certain operations that torch compile will not fine - Flash Attention\\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md': RefDocInfo(node_ids=['f69edc53-f475-4166-84e4-8e571236891d'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', 'filename': 'OpenAI API.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md': RefDocInfo(node_ids=['f68eb60a-3eb0-46be-8ba5-9979566daac6'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', 'filename': 'AdamW.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md': RefDocInfo(node_ids=['b74f605d-219a-408c-b7e8-cc36f2126221', '61036acc-861a-4e0b-b12c-431e8f3f3418'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md': RefDocInfo(node_ids=['df5f7349-8553-47a0-b3d8-4cfece7999c0', '597705e4-9982-4b05-8513-d2925adc7258', 'c8ca6285-796a-42c0-99fe-7dd9ddea04e8', 'b12069bf-0850-4ace-8c36-7885d32fa91a', 'c1be3add-c155-4eba-bd84-216f585c03f1', 'ceb5f3c1-51c7-4c94-80c7-8968c0e69914', '511d6839-bb37-4dc0-836a-761360e54a97'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md': RefDocInfo(node_ids=['e2ab8549-8b19-45bb-bb1f-9f7506864d4b'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md', 'filename': 'How Do Vision Transformers work?.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md': RefDocInfo(node_ids=['52641d5c-49d6-4d46-81fd-5a98f1062cde', '4cc777ee-6146-4821-884d-20354df91ecd', '67ea54dc-b723-4fbc-ac7a-673ad7442345', '83294dac-b95b-418a-97c9-8b90c3a31dd3', '18ad8218-6e19-4628-b96d-fe8815e67e0e', '49b1b018-d51a-480d-8afa-e1e90dc503df', '480de224-690a-4e96-94a3-4359991c6634', '49bab0f1-2b39-4b9a-8ef7-14a30b94833a', '6c244109-849a-4f21-8402-5c886dacc4e5', '82ff0f52-d210-474a-ae55-3af8e4789e78', 'b2c143cd-25a8-4c78-b5ed-89ddc7bd3ac3', '6b7e44a8-2212-4cca-ac45-a0268f694954', 'ef4d3327-68db-468f-a946-ed4ea395b86e', '7197cbdb-fe2a-44b8-848b-f8c69083f334', '2625eaf3-5eef-4e61-9429-c1cfb99a58ee'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'})}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docstore.get_all_ref_doc_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = \"What do we understand about Quantization?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"Quantization is a process aimed at reducing the precision of a model's parameters by converting them from higher bit-width representations, such as 32-bit floating point, to lower bit-width formats, like 8-bit integers. This reduction in precision helps to decrease the memory requirements of a model, which can be estimated using the formula that incorporates the number of bits and the number of parameters.\\n\\nThere are different types of quantization, one of which is asymmetric quantization. In this method, the mapping is not symmetric around zero; instead, it aligns the minimum and maximum values from the floating-point range to the corresponding values in the quantized range. Key calculations involved in this process include determining the scale (S) and zero point (Z), which are essential for converting between the quantized and dequantized values.\", source_nodes=[NodeWithScore(node=TextNode(id_='13373866-1a20-459c-b1f5-82fcd8f8deb4', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='899d90fed4c3a71fb1ae7cfd6c876f0cd012ebd79a54601eb474c4f129f709f1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7102c34f-29f2-4006-9035-5a9d1442904d', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='b19e2944f0add4ac6526e9968bbfc232c704dfe5ea04437d551e06fcc653f9dc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='117860b5-68f9-48bf-ae25-54df36d3776d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='1664098a1f095e1af315a00accb7a05ae47c29ea21d4b049768bdde372610786')}, text=\"Introduction\\nhttps://substack.com/inbox/post/145531349\\n\\n\\n\\n- The more bits we have available, the larger the range of values that can be represented.\\n- The interval of representable numbers of a given representation can take is called the dynamic range and the distance between two neighboring values is called precision\\n\\n\\n\\n\\nIt's possible to estimate the memory required by a model by it's precision and number of parameters\\n\\n$$\\nmemory = \\\\frac{nr_{bits}}{8} * nr_{params}\\n$$\\n\\nQuantization aims to reduce the precision of a model's parameter form higher bit-widths (like 32-bit floating point) to lower bit-widths (like 8-bit integers)\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6355641462505198), NodeWithScore(node=TextNode(id_='2325690d-843e-4811-b64a-1d71c9a9efe5', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='2809748f20962174e32cbd831756b0b6e2b35b9822aa29b6b03e88f11fb60f31'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='23da06b2-daf0-4eb3-9ea5-51b176d86334', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='0d74933abf0ca80d5adf2dc14d30bcd10aa9af962244ffbe75528db5b5ef3346'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='051b31aa-0aa1-40dc-b9f8-de13b7820f40', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f004fa2e1bc33c9fbc9b5ccd9dbacfbb60c16369d9ca436bda7418f5efbfa284')}, text='Asymmetric Quantization\\n\\n- This quantization is not symmetric around zero\\n- Instead, it maps the minimum (**β**) and maximum (**α**) values from the float range to the minimum and maximum values of the quantized range\\n\\n\\n$$\\nS = \\\\frac{128 - (-127)}{\\\\alpha - \\\\beta}\\n$$\\n$$\\nZ = round(-S *\\\\beta) - 2^{b-1} (Zero Point)\\n$$\\n$$\\nX_{quantized}= round (S.X + Z)\\n$$\\n\\n$$\\nX_{dequantized} = round(S * X_{quantized} + Z)\\n$$', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6322960892832064)], metadata={'13373866-1a20-459c-b1f5-82fcd8f8deb4': {'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, '2325690d-843e-4811-b64a-1d71c9a9efe5': {'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Different quantization methods include:\\n\\n1. **Symmetric Quantization**\\n   - This method is centered around zero and uses the same scale factor for both positive and negative values.\\n\\n2. **Asymmetric Quantization**\\n   - This method does not center around zero and maps the minimum and maximum values from the float range to the quantized range.\\n\\n3. **Dynamic Quantization**\\n   - A post-training method that adjusts the quantization parameters dynamically based on the input data.\\n\\n4. **Static Quantization**\\n   - Another post-training method that uses a fixed set of quantization parameters determined from a representative dataset.\\n\\n5. **Quantization Aware Training (QAT)**\\n   - A training method that incorporates quantization into the training process to improve model performance post-quantization.', source_nodes=[NodeWithScore(node=TextNode(id_='7102c34f-29f2-4006-9035-5a9d1442904d', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='226e28153bb7573d2be113fbf3b20c30dcd05d6f414e302c9fb12fce200df369'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='13373866-1a20-459c-b1f5-82fcd8f8deb4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b295995d44fe9890d418331e24eea73663904f08751611fa3293680c5aa90758')}, text='- [[#Introduction|Introduction]]\\n- [[#Common Data Types|Common Data Types]]\\n- [[#Squeezing Methods|Squeezing Methods]]\\n\\t- [[#Squeezing Methods#Symmetric Quantization|Symmetric Quantization]]\\n\\t\\t- [[#Symmetric Quantization#AbsMax Quantization|AbsMax Quantization]]\\n\\t\\t- [[#Symmetric Quantization#Asymmetric Quantization|Asymmetric Quantization]]\\n- [[#Range Mapping and Clipping|Range Mapping and Clipping]]\\n- [[#Calibration|Calibration]]\\n\\t- [[#Calibration#Calibrating Weights|Calibrating Weights]]\\n\\t- [[#Calibration#Calibrating Activations|Calibrating Activations]]\\n- [[#Post-Training Quantization|Post-Training Quantization]]\\n\\t- [[#Post-Training Quantization#1. Dynamic Quantization|1. Dynamic Quantization]]\\n\\t- [[#Post-Training Quantization#2. Static Quantization|2. Static Quantization]]\\n- [[#Quantization Aware Training (QAT)|Quantization Aware Training (QAT)]]', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6196110820203143), NodeWithScore(node=TextNode(id_='2325690d-843e-4811-b64a-1d71c9a9efe5', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='2809748f20962174e32cbd831756b0b6e2b35b9822aa29b6b03e88f11fb60f31'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='23da06b2-daf0-4eb3-9ea5-51b176d86334', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='0d74933abf0ca80d5adf2dc14d30bcd10aa9af962244ffbe75528db5b5ef3346'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='051b31aa-0aa1-40dc-b9f8-de13b7820f40', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f004fa2e1bc33c9fbc9b5ccd9dbacfbb60c16369d9ca436bda7418f5efbfa284')}, text='Asymmetric Quantization\\n\\n- This quantization is not symmetric around zero\\n- Instead, it maps the minimum (**β**) and maximum (**α**) values from the float range to the minimum and maximum values of the quantized range\\n\\n\\n$$\\nS = \\\\frac{128 - (-127)}{\\\\alpha - \\\\beta}\\n$$\\n$$\\nZ = round(-S *\\\\beta) - 2^{b-1} (Zero Point)\\n$$\\n$$\\nX_{quantized}= round (S.X + Z)\\n$$\\n\\n$$\\nX_{dequantized} = round(S * X_{quantized} + Z)\\n$$', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6072885127816289)], metadata={'7102c34f-29f2-4006-9035-5a9d1442904d': {'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, '2325690d-843e-4811-b64a-1d71c9a9efe5': {'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query('What are different quantisation methods?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_doc_ref = '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md'\n",
    "index.delete_ref_doc(quant_doc_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Different quantization methods include:\\n\\n1. **TF32 (Tensor Float 32)**: This method reduces the precision of float 32 computations by truncating the mantissa to 19 bits during computation, while maintaining float 32 for data movement across kernels and devices. It is supported on certain NVIDIA GPUs like the A100.\\n\\n2. **BFloat16**: This format has the same range as TF32 and float 32 but drops more precision in the mantissa, allowing for more aggressive quantization.\\n\\n3. **FP16 (Float 16)**: Initially used in the Volta series, this method requires gradient scaling to manage the reduced precision effectively.\\n\\nThese methods are designed to optimize performance while managing the trade-offs between precision and computational efficiency.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_engine.query('What are different quantisation methods?')\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: 1e83891c-0776-47c8-9b32-ff0be2727f14): Using Tensor Cores - TF 32\n",
      "Using FP32 and TF32 - In TF32, the inputs and outputs are float 32. Bu...\n",
      "\n",
      "> Source (Doc id: 09e470dd-c0f4-48f8-8b73-22cb9ddc3478): - ^Character level language Modelling - The basic unit is a character ^a60c01\n",
      "\t- Therefore a much...\n"
     ]
    }
   ],
   "source": [
    "print(response.get_formatted_sources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='a2a293f2-4fd2-43d3-8a52-a0dc1d6ed2f7', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', 'filename': 'OpenAI API.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', 'filename': 'OpenAI API.md'}, hash='486ae2453b11e0c560e10a8d1b0329f5bf06b89269cfa313b75291b754ff7027')}, text='azure models of interest - \\n- text-embedding-3-small - **text-embedding-3-small**,\\xa0**1**\\n- **dall-e-3**,\\xa0**3.0**\\n\\n- Essentially, with Azure openai api, you need to first deploy the models and then you can use them\\n- dalle-3 is used for image generation and not gpt4o-mini or gptx models\\n-', mimetype='text/plain', start_char_idx=0, end_char_idx=288, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_document refers to the node id\n",
    "index.docstore.get_document(\"a2a293f2-4fd2-43d3-8a52-a0dc1d6ed2f7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 61036acc-861a-4e0b-b12c-431e8f3f3418\n",
      "Text: Summary  - A webtext dataset is generated using reddit outbound\n",
      "links with at least 3 karma - this implies (implicitly) that the data\n",
      "is curated         - About 40 gb data set size with 8 million\n",
      "documents - The tokenization is [[#^a60c01]]byte pair encoding (BPE).\n",
      "It is a middle ground between word-level and character level encoding.\n",
      "It iteratively me...\n"
     ]
    }
   ],
   "source": [
    "print(index.docstore.get_document(\"61036acc-861a-4e0b-b12c-431e8f3f3418\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleDocumentStore' object has no attribute 'get_doc_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_doc_id\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m61036acc-861a-4e0b-b12c-431e8f3f3418\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleDocumentStore' object has no attribute 'get_doc_id'"
     ]
    }
   ],
   "source": [
    "index.docstore.get_doc_id(\"61036acc-861a-4e0b-b12c-431e8f3f3418\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a2a293f2-4fd2-43d3-8a52-a0dc1d6ed2f7': TextNode(id_='a2a293f2-4fd2-43d3-8a52-a0dc1d6ed2f7', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', 'filename': 'OpenAI API.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', 'filename': 'OpenAI API.md'}, hash='486ae2453b11e0c560e10a8d1b0329f5bf06b89269cfa313b75291b754ff7027')}, text='azure models of interest - \\n- text-embedding-3-small - **text-embedding-3-small**,\\xa0**1**\\n- **dall-e-3**,\\xa0**3.0**\\n\\n- Essentially, with Azure openai api, you need to first deploy the models and then you can use them\\n- dalle-3 is used for image generation and not gpt4o-mini or gptx models\\n-', mimetype='text/plain', start_char_idx=0, end_char_idx=288, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'bdf9ed6e-1a53-4724-a236-cb2448cbe21e': TextNode(id_='bdf9ed6e-1a53-4724-a236-cb2448cbe21e', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', 'filename': 'AdamW.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', 'filename': 'AdamW.md'}, hash='e728c471e04692930df982fd825732f4d5c05279cc7e2fd53a13256bab2b78e3')}, text='AdamW updates the weight decay part of adam\\n- Weight decay is a regularisation applied to large weights in the NN to move them towards zero\\n\\t- low weight decay could cause over-fitting\\n\\t- high weight decay could cause under-fitting', mimetype='text/plain', start_char_idx=1, end_char_idx=232, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '09e470dd-c0f4-48f8-8b73-22cb9ddc3478': TextNode(id_='09e470dd-c0f4-48f8-8b73-22cb9ddc3478', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, hash='5c60e7ff9f8d0482167e19c6833987cab89d54d780b24698df9b502ee730d28f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d954d17b-3ba6-48a2-aa01-6be28cf3f0b9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='52c1777ffba88e27ebcf0eb1dea3786d2bf1581f6476aed6ddd3e46f0c3807a8')}, text='- ^Character level language Modelling - The basic unit is a character ^a60c01\\n\\t- Therefore a much smaller vocabulary size\\n\\t- \\'The cat sat on the m\\' - the prediction would the character \\'a\\' to get to the word mat for this language model\\n\\t- Predicts on character level\\n\\t- Can handle out of vocabulary words and misspellings easily\\n- Word level language Modelling - The basic unit is a word\\n\\t- Much bigger vocabulary size\\n\\t- Output is much more coherent\\n\\n> Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively **interpolates between word level inputs for frequent symbol sequences and char- acter level inputs for infrequent symbol sequences.**\\n\\nClaude Explanation for Byte Pair\\n\\n\\n1. Character-level vs. Word-level Language Modeling:\\n\\nCharacter-level Language Modeling:\\n- Operates at the level of individual characters (letters, numbers, punctuation)\\n- Predicts the next character in a sequence\\n- Has a smaller vocabulary size (typically 26 letters, 10 digits, and some special characters)\\n- Can handle out-of-vocabulary words and misspellings more easily\\n\\nExample:\\nInput: \"The cat sat on the m\"\\nTask: Predict the next character (likely \"a\" to form \"mat\")\\n\\nWord-level Language Modeling:\\n- Operates at the level of whole words\\n- Predicts the next word in a sequence\\n- Has a larger vocabulary size (typically tens or hundreds of thousands of words)\\n- Generally produces more coherent output for larger contexts\\n\\nExample:\\nInput: \"The cat sat on the\"\\nTask: Predict the next word (e.g., \"mat\", \"chair\", \"roof\")\\n\\n2. Byte Pair Encoding (BPE):\\n\\nByte Pair Encoding, as explained in the GPT-2 paper \"Language Models are Unsupervised Multitask Learners,\" is a subword tokenization method that bridges the gap between character-level and word-level modeling.\\n\\nKey concepts:\\na) It starts with a vocabulary of individual characters and iteratively merges the most frequent pair of consecutive bytes or characters.\\nb) This process creates a vocabulary of subword units, which can represent both common words and parts of less common words.\\n\\nHow it works:\\n1. Initialize the vocabulary with individual characters.\\n2. Count the frequency of adjacent pairs of symbols in the training data.\\n3. Merge the most frequent pair and add it to the vocabulary.\\n4. Repeat steps 2-3 for a predetermined number of merges or until a desired vocabulary size is reached.\\n\\nExample:\\nLet\\'s say we have the following text: \"The dancer dances beautifully\"\\n\\nInitial vocabulary: [\\'T\\', \\'h\\', \\'e\\', \\' \\', \\'d\\', \\'a\\', \\'n\\', \\'c\\', \\'r\\', \\'s\\', \\'b\\', \\'u\\', \\'t\\', \\'i\\', \\'f\\', \\'l\\', \\'y\\']\\n\\nAfter some merges:\\n[\\'The\\', \\' \\', \\'d\\', \\'an\\', \\'c\\', \\'er\\', \\' \\', \\'d\\', \\'an\\', \\'ces\\', \\' \\', \\'b\\', \\'eau\\', \\'t\\', \\'i\\', \\'f\\', \\'ul\\', \\'ly\\']\\n\\nBenefits of BPE in the context of GPT-2:\\n1. Adaptability: It can handle both common and rare words effectively.\\n2. Efficiency: It creates a compact vocabulary that balances the trade-offs between character-level and word-level models.\\n3. Handling of unknown words: It can often break down unknown words into meaningful subword units.\\n\\nBy using BPE, GPT-2 achieves a balance between the flexibility of character-level models and the semantic coherence of word-level models. This allows it to work effectively across a wide range of languages and domains without requiring language-specific preprocessing or tokenization.\\n\\nWould you like me to elaborate on any specific aspect of these concepts?\\n\\n\\n\\n\\n\\n[[Metrics#Perplexity]]\\n\\nGPT 2 is trained on web text but evaluated on different datasets - children\\'s book test, lambada, etc without any fine tuning - Zero Shot Setting.\\n\\n\\nThe recommended way for creating train and test-splits for this dataset is to use \\n\\nZero shot performance could be used as baseline for performance for fine-tuning', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'd954d17b-3ba6-48a2-aa01-6be28cf3f0b9': TextNode(id_='d954d17b-3ba6-48a2-aa01-6be28cf3f0b9', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, hash='09056d55105357fb5aec473d7627dcd3418650682eb5943ed89b51c65122e7d3'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='09e470dd-c0f4-48f8-8b73-22cb9ddc3478', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, hash='009b8d51856f80c95fee8cf2f59b4ea3918a4d764a79ee699b4b4f49370267b6')}, text=\"Summary\\n\\n- A webtext dataset is generated using reddit outbound links with at least 3 karma - this implies (implicitly) that the data is curated\\n\\t- About 40 gb data set size with 8 million documents\\n- The tokenization is [[#^a60c01]]byte pair encoding (BPE). It is a middle ground between word-level and character level encoding. It iteratively merges various bytes (creating byte pairs/tokens) based on their frequency evaluated from the dataset into tokens\\n\\t- From Kaparthy's video - this is entire separate module of LMs. The LMs never see the actual text, they only see the tokens. Very important part of LMs.\\n\\t- This type of encoding allows for working with non-frequent word pairs as well and working with different languages.\\n- Evaluated on different datasets with a zero shot setting - Used web text as the dataset for training, then tested on different dataset without fine tuning. \\n- Use bloom filter of 8-grams data structure to identify duplicates and to create train and test splits.\", mimetype='text/plain', start_char_idx=2, end_char_idx=998, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'ccfdb41f-600b-4726-ae5d-7d4ee5dc5823': TextNode(id_='ccfdb41f-600b-4726-ae5d-7d4ee5dc5823', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='2334728aa0e4524b8242c38ec5d89c99b54344975bfcff4a6f6d97c2bc855e56'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c9ae64b6-6f4d-439f-b2aa-1854756d2cae', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='70d436e34f2a15d4daf3d6d0e849b0c7c198c28b788aea491e1264ce6b0897c4')}, text='- When checking the weight initialisation, check if the model is not favouring any particular class\\n\\t- The average expected loss could be computed when the model is initialised\\n\\t- For example, if we are using cross entropy loss and we have 50000 tokens, the token probabilities must be close to 1/50000 = 0.00002 and cross entropy loss should then be\\n\\t\\t-log(1/50000) = 10.8\\n\\t- Note above that cross entropy for single label will amount to - $-y_{gt}log(y_{pred})$  and $y_{gt}$ is $1$ and $y_{pred}$ is expected to be $1/50000$\\n- Overfit on a single batch\\n\\t- To check if your model is capable of learning, overfit on a single batch.\\n\\t- You must use only one batch, train your model for some epochs and then verify if the loss is decreasing over epochs (overfitting) to the batch\\n\\n- Transformer - Input and Output embeddings are the same\\n\\t- This is a type of inductive bias\\n\\t-  You want semantically similar words/tokens (lower-case\\\\==upper-case, synonyms etc) to have similar embeddings (at input) and probabilities (at output)\\n\\t- Also this makes the network very efficient for training and inference.\\n\\n- Weight initialisation\\n\\t- When using residual layers, we keep adding the activations to the input. We need to account for this during initialisation\\n\\t- As we keep adding inputs, the std keeps growing over the layers. We need to therefore scale the weight initialisation with respect to the number of residual layers in the network\\n\\n`torch.cuda.synchronize` - When we use torch, we actually scheduling a set of instructions on the GPU. When we use the above function, we essentially wait for the GPU to finish processing these set of instructions on the GPU and therefore synchronise the CPU and the GPU.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'c9ae64b6-6f4d-439f-b2aa-1854756d2cae': TextNode(id_='c9ae64b6-6f4d-439f-b2aa-1854756d2cae', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='316c92b6d923e492c5757c0890cf95b35e3e7c1b7ba7ee56d44d143331efd9d3'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ccfdb41f-600b-4726-ae5d-7d4ee5dc5823', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='eee02f912b29dcdf12785dcb4ade6ba1abe1033d1a4cd9e59f5fa1e146aa9997'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1e83891c-0776-47c8-9b32-ff0be2727f14', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2ac6fed8aca3a6ef437ee48bce6479715f3c5b9f0f85c591f4423250b093e2fe')}, text='Optimizations', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '1e83891c-0776-47c8-9b32-ff0be2727f14': TextNode(id_='1e83891c-0776-47c8-9b32-ff0be2727f14', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='b7c95ce6207c0bf35a60da392830943e890937ab399659daf724ee5e9312a94b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c9ae64b6-6f4d-439f-b2aa-1854756d2cae', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='4a49dae1b27551009c968d7ceb3cee8774e2cdba038ff99b2b5ce2ab1734d181'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8180bbff-676f-45ec-9b77-4aecfc1fb807', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='441b2b239e90e925528c20cce5a1b1b335265361800ee54de3d802e0a6e6a414')}, text=\"Using Tensor Cores - TF 32\\nUsing FP32 and TF32 - In TF32, the inputs and outputs are float 32. But **during the computation**, the 32 bit is reduced to 19 bits (the truncation happens in the mantissa bits \\\\[13 bits are dropped]) for computation. This is supported on certain NVIDIA GPUS like A100. Note that TF 32 is performed on tensor cores and only the computation is done in TF 32. The movement of data across different kernels and devices is still Float 32.\\n\\n\\n\\n\\nYou can set this in torch using `torch.set_float32_matmul_precision('high') #tf32`\\n\\n\\nBFloat16 - has the same range as TF 32 and float 32, but much more aggressive in dropping the precision/mantissa part of the floating point.\\n\\n---\\nThe range can be understood with an example - the range of a certain type could be from -100 to 100. The range is about 201 different numbers. If the precision of two types is different, then it comes down to how many decimal points for each number can be presented. for example type 1 could represent a number like 67.12311 and type 2 could only represent a number up to 67.1. \\n\\n> The above is just an example\\n\\n---\\n\\n\\nFp16 came first for Volta series but requires gradient scaling. BFloat 16\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '8180bbff-676f-45ec-9b77-4aecfc1fb807': TextNode(id_='8180bbff-676f-45ec-9b77-4aecfc1fb807', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='30f3edbe7421f44fbe76b9c67acd14169dc0eebe606e55f857a181fe5dea3f6a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1e83891c-0776-47c8-9b32-ff0be2727f14', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='b5874db5e8527bbbe181c17fb38b8524d81fa5096fa9977b2de752399f3a59a8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fffbc905-d1d0-4476-b36a-9ed3daea271b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='68531443b55e90eb7826dc8a87a4b886081b0f20df71f9cf73689896db669a71')}, text=\"torch.compile\\n\\n- Think of torch.compile as gcc for neural nets\\n\\nWhen you provide the model to torch.compile, sees your entire code at run time, sees the operations that you want to run and optimize process. \\n- Remove the python interpreter from the forward pass entirely (this also means that you won't be able to debug while using this)\\n- It will compile the NN into a single object\\n- GPU read/writes are optimised.  One way is kernel fusion. Obviously you need to know what you are going to compute before hand for kernel fusion.\\n\\n\\t> Kernel fusion is essentially combining multiple operations into 1 or fewer operations. This ensures that loading input and output from memory to cores is significantly decreases as only one kernel operations need to be performed instead of multiple.\\n\\t> \\n\\t> The intermediate calculations reside on the chip in SRAM (which is usually very small). \\n\\t> \\n\\t> If multiple kernels are used, the input is loaded to cores and then returned back to memory and then this taken by the next kernel and so on. This travelling back and forth between memory (GPU read/write) and GPU cores is expensive.\\n\\n-  There are certain operations that torch compile will not fine - Flash Attention\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'fffbc905-d1d0-4476-b36a-9ed3daea271b': TextNode(id_='fffbc905-d1d0-4476-b36a-9ed3daea271b', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='a4d45fb349df059e6b13384d1939599c8e3e687583d912223e575fb9f5509acd'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8180bbff-676f-45ec-9b77-4aecfc1fb807', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='9db47491a313a546ff02f13cc7da604087ba61d7e176d5b648b0a01b1a202e4e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d487c958-c9e5-4b5f-a15f-6efa9be06757', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ca4acf7c1df3010820b954b2537a58b810fa55dea28c11d662f8639e40e1dc97')}, text='Flash Attention\\n\\n- An algorithm to run attention operations a lot faster. Implementing the following lines very efficiently\\n```\\nattn = ((q @ k.transpose(-1, -2)) * 1 / math.sqrt(self.n_embed)) # b, n_head, t,t\\nattn = attn.masked_fill(self.bias[:, :, :t, :t] == 0, float(\"-inf\"))\\nattn = F.softmax(attn, dim=-1)\\ny = (attn @ v) # (b, n_head, t, t) * (b, n_head, t, c/head) -> b, n_head, t, c/head\\n```\\n\\nThis is a kernel fusion operation\\n\\n\\nFlash attention is very mindful of the memory architecture in the GPU and therefore can perform almost 8x faster than the attention implementation above. (Fewer reads and writes to High Bandwidth memory (HBM))\\n\\nThis is based on an efficient way to compute Softmax calculation. Fuse all other calculations of attention with this Softmax calculation for faster attention.\\n\\nInterestingly, Flash Attention has more flops than regular attention. This displays that the limiting GPU reads/writes from and to HBM are much more important faster faster computation.\\n\\nIn PyTorch, just use this for flash attention\\n```\\ny = F.scaled_dot_product_attention(q, k, v, is_causal=True)\\n```', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'd487c958-c9e5-4b5f-a15f-6efa9be06757': TextNode(id_='d487c958-c9e5-4b5f-a15f-6efa9be06757', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='943d08715248bfd9dac0c5a1e0635c76700c3fbfab5669651c4574b07803cdaa'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fffbc905-d1d0-4476-b36a-9ed3daea271b', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='fdbdf50fda29a4c01d673f15e8d0ac424cf0476460291fca46019c9f01bbf60c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b4b10781-600f-454d-bea3-e9c4079a4ae1', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b5986cd52c23350734ba0d2a6473d868e8593f3e81c9fca87bd1df92c1974d5d')}, text='Powers of 2\\n\\n- Everything in cuda works in powers of 2 and lots of kernels are written to keep in mind powers of 2.\\n- Calculations works well in chunks of powers of 2. Otherwise - it could be inefficient.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'b4b10781-600f-454d-bea3-e9c4079a4ae1': TextNode(id_='b4b10781-600f-454d-bea3-e9c4079a4ae1', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='f4938ba45c54824f02f925d35d1d74e9e2270c2d633d9277c9bf9299f24c6746'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d487c958-c9e5-4b5f-a15f-6efa9be06757', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='e4fd0b3e60f9f2c80e398582f14a996967362e3a07b05c12b8136e373ea04198')}, text=\"Gradient Accumulation\\n- You can use gradient accumulation to use any arbitrary batch size \\n- Remember to normalise the loss values with the grad accumulation steps - because the averaging of loss over a batch that happens without accumulation doesn't happen. To correctly replicated, divide the loss.\", mimetype='text/plain', start_char_idx=2, end_char_idx=302, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '6e644d9d-bea8-4006-99ff-ea9d92da463a': TextNode(id_='6e644d9d-bea8-4006-99ff-ea9d92da463a', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md', 'filename': 'How Do Vision Transformers work?.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md', 'filename': 'How Do Vision Transformers work?.md'}, hash='bd95eecf4217eb1d480b6604f4e8992c6c3a45207d04f83a5c0e0149ee129bf9')}, text='- When data set is small, ViT have been known to overfit\\n- They are good for long range dependecies\\n\\n\\n- ViTs are robust against Data Corruption, image occlusions and adversarial attacks\\n- MSAs closer to last layer significantly improve predictive performance.\\n\\n- ViTS are low pass filters - learn shape better, while CNNs are high pass filters - learn texture better. This is why ViTs are not vulnerable to high-frequency noise.\\n\\t- Therefore VITs and CNNS are complementary', mimetype='text/plain', start_char_idx=1, end_char_idx=474, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '7102c34f-29f2-4006-9035-5a9d1442904d': TextNode(id_='7102c34f-29f2-4006-9035-5a9d1442904d', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='226e28153bb7573d2be113fbf3b20c30dcd05d6f414e302c9fb12fce200df369'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='13373866-1a20-459c-b1f5-82fcd8f8deb4', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b295995d44fe9890d418331e24eea73663904f08751611fa3293680c5aa90758')}, text='- [[#Introduction|Introduction]]\\n- [[#Common Data Types|Common Data Types]]\\n- [[#Squeezing Methods|Squeezing Methods]]\\n\\t- [[#Squeezing Methods#Symmetric Quantization|Symmetric Quantization]]\\n\\t\\t- [[#Symmetric Quantization#AbsMax Quantization|AbsMax Quantization]]\\n\\t\\t- [[#Symmetric Quantization#Asymmetric Quantization|Asymmetric Quantization]]\\n- [[#Range Mapping and Clipping|Range Mapping and Clipping]]\\n- [[#Calibration|Calibration]]\\n\\t- [[#Calibration#Calibrating Weights|Calibrating Weights]]\\n\\t- [[#Calibration#Calibrating Activations|Calibrating Activations]]\\n- [[#Post-Training Quantization|Post-Training Quantization]]\\n\\t- [[#Post-Training Quantization#1. Dynamic Quantization|1. Dynamic Quantization]]\\n\\t- [[#Post-Training Quantization#2. Static Quantization|2. Static Quantization]]\\n- [[#Quantization Aware Training (QAT)|Quantization Aware Training (QAT)]]', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '13373866-1a20-459c-b1f5-82fcd8f8deb4': TextNode(id_='13373866-1a20-459c-b1f5-82fcd8f8deb4', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='899d90fed4c3a71fb1ae7cfd6c876f0cd012ebd79a54601eb474c4f129f709f1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7102c34f-29f2-4006-9035-5a9d1442904d', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='b19e2944f0add4ac6526e9968bbfc232c704dfe5ea04437d551e06fcc653f9dc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='117860b5-68f9-48bf-ae25-54df36d3776d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='1664098a1f095e1af315a00accb7a05ae47c29ea21d4b049768bdde372610786')}, text=\"Introduction\\nhttps://substack.com/inbox/post/145531349\\n\\n\\n\\n- The more bits we have available, the larger the range of values that can be represented.\\n- The interval of representable numbers of a given representation can take is called the dynamic range and the distance between two neighboring values is called precision\\n\\n\\n\\n\\nIt's possible to estimate the memory required by a model by it's precision and number of parameters\\n\\n$$\\nmemory = \\\\frac{nr_{bits}}{8} * nr_{params}\\n$$\\n\\nQuantization aims to reduce the precision of a model's parameter form higher bit-widths (like 32-bit floating point) to lower bit-widths (like 8-bit integers)\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '117860b5-68f9-48bf-ae25-54df36d3776d': TextNode(id_='117860b5-68f9-48bf-ae25-54df36d3776d', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='356f61aa5e5067fc0cd34c2285fe8280a357c172285b8e729e93fde9e9b475ba'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='13373866-1a20-459c-b1f5-82fcd8f8deb4', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='7315e8422c14551f3cebee5662d9d520cfd3d45bd04318a13c99f4eb406a428c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='dcc6ff20-8488-4dc6-a89f-ebb5c3687a4e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='995af9afc013e00186ffcc810c8d215c125f2fbca006d7217ebf0d3f2d0a6ba0')}, text=\"Common Data Types\\n\\n1. FP 16\\n2. BF 16 - Same dynamic range as FP 32 but bits equal to FP 16\\n3. INT 8\\n\\n\\n- In practice, we don't need to map the entire dynamic range of FP 32, but only the range of our data (our weights) to a lower precision\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'dcc6ff20-8488-4dc6-a89f-ebb5c3687a4e': TextNode(id_='dcc6ff20-8488-4dc6-a89f-ebb5c3687a4e', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='d780ba2e82aeb71f52a3062c71e803930284703997ab71fba8ffcc2199652729'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='117860b5-68f9-48bf-ae25-54df36d3776d', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='8d0406db03dc641ff3e4f306c822a7e84005e8bc90589ac5ee9ed32d4b7b5f6c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fd428694-a6d0-4e54-b1ce-c79ade8748c8', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='1ac608a6969d7693278cc0f70997bd6d92212d720a4dd23fe0ab55a177f700a9')}, text='Squeezing Methods', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'fd428694-a6d0-4e54-b1ce-c79ade8748c8': TextNode(id_='fd428694-a6d0-4e54-b1ce-c79ade8748c8', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='27a1d386b19d168c9af9b77738a66d41001bb275e98d03c6ae4c1fb844bfea0d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='dcc6ff20-8488-4dc6-a89f-ebb5c3687a4e', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='b2d064f73670356e336a297120d4aaf5db0acb46244f612c7afb417e428cdd0b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='23da06b2-daf0-4eb3-9ea5-51b176d86334', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='8e3d6f8ab5142d7f5e74ad29a5765fe8731bd02b616803810ed24fe29298527b')}, text='Symmetric Quantization\\n\\n- The range of of original FP values is mapped to symmetric range around zero in the quantized space.  Zero in original FP space is equal to zero in Quantized space', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '23da06b2-daf0-4eb3-9ea5-51b176d86334': TextNode(id_='23da06b2-daf0-4eb3-9ea5-51b176d86334', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='457213841e690b8fbb9221520c426975bf253eeb271fe6b40edfcf370988ab90'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fd428694-a6d0-4e54-b1ce-c79ade8748c8', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='413aee19a81ccf31c881ba6f9df39577ab997d0448793ef112f897a15b8fcf8e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2325690d-843e-4811-b64a-1d71c9a9efe5', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ce3e953b068f4824e356054a900d94faa6f05faa587ebcf086425f4ad2657a37')}, text='AbsMax Quantization\\n\\nWe take the highest absolute value ($\\\\alpha$) in our data to perform the linear mapping\\n\\n\\n\\n$$\\nS = \\\\frac{2^b - 1}{\\\\alpha} (Scale Factor)\\n$$\\n\\n$$\\nX_{quantized} = round(S.X)\\n$$\\n\\n\\n\\n\\n\\n\\n$$\\nX_{dequantized} = \\\\frac{X_{quantized}}{S}\\n$$\\n\\n- When you convert the quantized value back to the dequantized value, the difference between the dequantized value and original value is called **quantization error**\\n\\t- The lower the number of bits, the more quantization we tend to have', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '2325690d-843e-4811-b64a-1d71c9a9efe5': TextNode(id_='2325690d-843e-4811-b64a-1d71c9a9efe5', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='2809748f20962174e32cbd831756b0b6e2b35b9822aa29b6b03e88f11fb60f31'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='23da06b2-daf0-4eb3-9ea5-51b176d86334', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='0d74933abf0ca80d5adf2dc14d30bcd10aa9af962244ffbe75528db5b5ef3346'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='051b31aa-0aa1-40dc-b9f8-de13b7820f40', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f004fa2e1bc33c9fbc9b5ccd9dbacfbb60c16369d9ca436bda7418f5efbfa284')}, text='Asymmetric Quantization\\n\\n- This quantization is not symmetric around zero\\n- Instead, it maps the minimum (**β**) and maximum (**α**) values from the float range to the minimum and maximum values of the quantized range\\n\\n\\n$$\\nS = \\\\frac{128 - (-127)}{\\\\alpha - \\\\beta}\\n$$\\n$$\\nZ = round(-S *\\\\beta) - 2^{b-1} (Zero Point)\\n$$\\n$$\\nX_{quantized}= round (S.X + Z)\\n$$\\n\\n$$\\nX_{dequantized} = round(S * X_{quantized} + Z)\\n$$', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '051b31aa-0aa1-40dc-b9f8-de13b7820f40': TextNode(id_='051b31aa-0aa1-40dc-b9f8-de13b7820f40', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='375bbfcd0c7c17bc927f5d9f6a461bec063804db9b34a04ac427f27bd3a77f25'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2325690d-843e-4811-b64a-1d71c9a9efe5', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='f7ad494da1f84c19a1fd3c00a66e0bc6924b3fdc7065d4e03197a6ed31be8d42'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fb55c447-1a90-4dda-b0ae-e688d23ecf1d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0d7609e0440bf83c3f6fc269dce9ed94013a8c4f0a818e0ab2eb9c4b47888774')}, text='Range Mapping and Clipping\\n- If there are any outliers, then most values will be clustered around or quantized to a single value. \\n\\n\\n\\n- Instead we can clip certain values - clipping is setting a different dynamic range of the original values to remove outliers\\n- **Quantization error of the non-outliers is reduced significantly**', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'fb55c447-1a90-4dda-b0ae-e688d23ecf1d': TextNode(id_='fb55c447-1a90-4dda-b0ae-e688d23ecf1d', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='5612533d3218bc343c4b2a80bfd70d4e6929d20c997067047b764a5baf3ba0f2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='051b31aa-0aa1-40dc-b9f8-de13b7820f40', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='3489f89052e51eaafe519ce526cdb17a4c5f193da060528ec8e36df7c60002d8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5d1e05c6-b9c4-4e6a-b7bc-5037f1e0318c', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3f78c94f5095b3dbb3e12ad043472e7522cf80267ff34ee17c8fe99adba2d55a')}, text='Calibration\\n\\nThe process of selecting the clipping values is called Calibration - to find as many values as possible within the range while minimising the quantization error.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '5d1e05c6-b9c4-4e6a-b7bc-5037f1e0318c': TextNode(id_='5d1e05c6-b9c4-4e6a-b7bc-5037f1e0318c', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='3f9ae079502968ca9cc92f4df179a9af968a9f5e8aa6e7719337b95b3f338c07'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='fb55c447-1a90-4dda-b0ae-e688d23ecf1d', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='f28a5423ee08a1ce82e0800e096412e6b4ee5fff5656086b96f0ad0bb289215c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='550ef65f-56a5-4470-851c-bac216e36db7', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='fbf9008bf5208f00360ddcb3b2471f50ed8928d23c8d4dafd634335139f29570')}, text='Calibrating Weights\\n\\nNote that weights are static - known before the model is run. Calibration is therefore easier\\n\\n- Choose percentile of the input range\\n- Optimize Mean Squared Error (MSE) between original and quantized weights\\n- Minimizing entropy (KL-Divergence) between the original and quantized values', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '550ef65f-56a5-4470-851c-bac216e36db7': TextNode(id_='550ef65f-56a5-4470-851c-bac216e36db7', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='1ed57e8f4490489df1ef7faf2b7d0c396aa92bd43467e85f5d8990bc1f2d7cf0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5d1e05c6-b9c4-4e6a-b7bc-5037f1e0318c', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='c1536b03a6c5a55ea49e674a1199946c2891aabb142948974946917875f736d0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ea41f0ac-9d33-4e15-aa6b-ecc0114bce8f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='325a30cd3c2218999bacbbeba659eb01ad12e1f5b1e266b57d080cb584f14073')}, text='Calibrating Activations\\n\\nActivations change after every input later - only known during inference\\n\\nWe use\\n- Post-Training Quantization (PTQ)\\n- Quantization Aware Training (QAT)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'ea41f0ac-9d33-4e15-aa6b-ecc0114bce8f': TextNode(id_='ea41f0ac-9d33-4e15-aa6b-ecc0114bce8f', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='e13035d3f413d4fe9eda18ca5f1d3e7b53252a835b5ca80311127a182af45556'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='550ef65f-56a5-4470-851c-bac216e36db7', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='3320a0fa9e4dafa9a589e4cf2588792eb2729f80547ed253bd267e28f8df330b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1a78e32c-7975-4d4a-9046-6bf86f94962f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b140feab5c9977930924bd50654d9e9991867392b1c9c56819f55ed4e8b9d30a')}, text='Post-Training Quantization\\n- Involved quantizing both weights and activations after training the model\\n- Quantization of weights is performed using symmetric or asymmetric quantization\\n- Quantization of activations is done by inferencing the model to get their potential distribution to find their range', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '1a78e32c-7975-4d4a-9046-6bf86f94962f': TextNode(id_='1a78e32c-7975-4d4a-9046-6bf86f94962f', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='227dbb72474313af4b8a62a0cfeb53d5fbe6cbb9317641a80dadc86b3eec31e5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ea41f0ac-9d33-4e15-aa6b-ecc0114bce8f', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='37781972d255eba9bbaa625488f2223b472430bb5187343343c8d963fdeae9cf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a26410be-da52-49eb-a0c8-a54c8a39ad3c', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2e1cbe7796b82c71d52bd62785235a6fcac4d004b2d8e8a1feceace05e39ba9a')}, text='1. Dynamic Quantization\\n\\n- Data is Passed through the NN and activations are collected\\n- Then the scale and zero-point is calculated for quantizations\\n- Each layer has its own scale and zero-point values\\n- The process is repeated each time data passes through a new layer', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " 'a26410be-da52-49eb-a0c8-a54c8a39ad3c': TextNode(id_='a26410be-da52-49eb-a0c8-a54c8a39ad3c', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='85b331ca6ed49d3ab4e6aa77eac29a2e3a8a801bdd795da8e4f1cc17844bf7b5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1a78e32c-7975-4d4a-9046-6bf86f94962f', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='dfd8f109836f019ee7a06a031175052a7b3ddc47762349454d1553073895d23f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='73b21100-bccf-4239-84cb-1dcc2d1d4541', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7f6228fc3bd30f5ff0b278ab5cf472f33a975984a38c6852b8637814abe57efc')}, text=\"2. Static Quantization\\nStatic Quantization doesn't calculate the zero-point and scale factor during inference but beforehand.\\n- A calibration dataset is used and given to the model to collect these potential distributions\\n- During inference, the s and z values are already pre-calculated and used globally over all activations to quantize them.\\n\\n\\n\\n\\nDynamic Quantization is slower but more accurate\\nStatic Quantization is faster but less accurate\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " '73b21100-bccf-4239-84cb-1dcc2d1d4541': TextNode(id_='73b21100-bccf-4239-84cb-1dcc2d1d4541', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='b119ea2ac6503328672d6d69f25d8f615c538da971d2c63ed0752770ae64a1d9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a26410be-da52-49eb-a0c8-a54c8a39ad3c', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'}, hash='8f6e08a07021453ac2370ff128a3cf892903c9b6762302c23f47734cabfbf190')}, text='Quantization Aware Training (QAT)\\nQAT aims to learn the quantization procedure during training.\\n\\n- During Quantization, so-called fake quants are introduced. This is the process of first quantizing the weights to lower precision and then de-quantizing back to FP 32.\\n\\n\\n- This allows the model to consider the quantization during training, loss calculation and weight updates.\\n\\n\\nQAT attempts to explore the loss landscape for “_wide_” minima to minimize the quantization errors as “_narrow_” minima tend to result in larger quantization errors.\\n\\n\\n!\\n\\nFor example, imagine if we did not consider quantization during the backward pass. We choose the weight with the smallest loss according to gradient descent. However, that would introduce a larger quantization error if it’s in a “_narrow_” minima.\\n\\nIn contrast, if we consider quantization, a different updated weight will be selected in a “_wide_” minima with a much lower quantization error.\\n\\n!\\n\\nAs such, although PTQ has a lower loss in high precision (e.g., FP32), QAT results in a lower loss in lower precision (e.g., INT4) which is what we aim for.', mimetype='text/plain', start_char_idx=2, end_char_idx=1105, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docstore.docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_add_nodes_to_index',\n",
       " '_adelete_from_docstore',\n",
       " '_adelete_from_index_struct',\n",
       " '_aget_node_with_embedding',\n",
       " '_async_add_nodes_to_index',\n",
       " '_build_index_from_nodes',\n",
       " '_callback_manager',\n",
       " '_delete_from_docstore',\n",
       " '_delete_from_index_struct',\n",
       " '_delete_node',\n",
       " '_docstore',\n",
       " '_embed_model',\n",
       " '_get_node_with_embedding',\n",
       " '_graph_store',\n",
       " '_index_struct',\n",
       " '_insert',\n",
       " '_insert_batch_size',\n",
       " '_object_map',\n",
       " '_show_progress',\n",
       " '_storage_context',\n",
       " '_store_nodes_override',\n",
       " '_transformations',\n",
       " '_use_async',\n",
       " '_vector_store',\n",
       " 'adelete_ref_doc',\n",
       " 'as_chat_engine',\n",
       " 'as_query_engine',\n",
       " 'as_retriever',\n",
       " 'build_index_from_nodes',\n",
       " 'delete',\n",
       " 'delete_nodes',\n",
       " 'delete_ref_doc',\n",
       " 'docstore',\n",
       " 'from_documents',\n",
       " 'from_vector_store',\n",
       " 'index_id',\n",
       " 'index_struct',\n",
       " 'index_struct_cls',\n",
       " 'insert',\n",
       " 'insert_nodes',\n",
       " 'ref_doc_info',\n",
       " 'refresh',\n",
       " 'refresh_ref_docs',\n",
       " 'set_index_id',\n",
       " 'storage_context',\n",
       " 'summary',\n",
       " 'update',\n",
       " 'update_ref_doc',\n",
       " 'vector_store']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_check_frozen',\n",
       " '_copy_and_set_values',\n",
       " '_data',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " 'aclear',\n",
       " 'add',\n",
       " 'adelete',\n",
       " 'adelete_nodes',\n",
       " 'aget_nodes',\n",
       " 'aquery',\n",
       " 'async_add',\n",
       " 'class_name',\n",
       " 'clear',\n",
       " 'client',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'custom_model_dump',\n",
       " 'data',\n",
       " 'delete',\n",
       " 'delete_nodes',\n",
       " 'dict',\n",
       " 'from_dict',\n",
       " 'from_json',\n",
       " 'from_namespaced_persist_dir',\n",
       " 'from_orm',\n",
       " 'from_persist_dir',\n",
       " 'from_persist_path',\n",
       " 'get',\n",
       " 'get_nodes',\n",
       " 'is_embedding_query',\n",
       " 'json',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'persist',\n",
       " 'query',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'stores_text',\n",
       " 'to_dict',\n",
       " 'to_json',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.vector_store.data.embedding_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md': RefDocInfo(node_ids=['f69edc53-f475-4166-84e4-8e571236891d'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', 'filename': 'OpenAI API.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md': RefDocInfo(node_ids=['f68eb60a-3eb0-46be-8ba5-9979566daac6'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', 'filename': 'AdamW.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md': RefDocInfo(node_ids=['b74f605d-219a-408c-b7e8-cc36f2126221', '61036acc-861a-4e0b-b12c-431e8f3f3418'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md': RefDocInfo(node_ids=['df5f7349-8553-47a0-b3d8-4cfece7999c0', '597705e4-9982-4b05-8513-d2925adc7258', 'c8ca6285-796a-42c0-99fe-7dd9ddea04e8', 'b12069bf-0850-4ace-8c36-7885d32fa91a', 'c1be3add-c155-4eba-bd84-216f585c03f1', 'ceb5f3c1-51c7-4c94-80c7-8968c0e69914', '511d6839-bb37-4dc0-836a-761360e54a97'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md': RefDocInfo(node_ids=['e2ab8549-8b19-45bb-bb1f-9f7506864d4b'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md', 'filename': 'How Do Vision Transformers work?.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md': RefDocInfo(node_ids=['52641d5c-49d6-4d46-81fd-5a98f1062cde', '4cc777ee-6146-4821-884d-20354df91ecd', '67ea54dc-b723-4fbc-ac7a-673ad7442345', '83294dac-b95b-418a-97c9-8b90c3a31dd3', '18ad8218-6e19-4628-b96d-fe8815e67e0e', '49b1b018-d51a-480d-8afa-e1e90dc503df', '480de224-690a-4e96-94a3-4359991c6634', '49bab0f1-2b39-4b9a-8ef7-14a30b94833a', '6c244109-849a-4f21-8402-5c886dacc4e5', '82ff0f52-d210-474a-ae55-3af8e4789e78', 'b2c143cd-25a8-4c78-b5ed-89ddc7bd3ac3', '6b7e44a8-2212-4cca-ac45-a0268f694954', 'ef4d3327-68db-468f-a946-ed4ea395b86e', '7197cbdb-fe2a-44b8-848b-f8c69083f334', '2625eaf3-5eef-4e61-9429-c1cfb99a58ee'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'})}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docstore.get_all_ref_doc_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md': RefDocInfo(node_ids=['f69edc53-f475-4166-84e4-8e571236891d'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/OpenAI API.md', 'filename': 'OpenAI API.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md': RefDocInfo(node_ids=['f68eb60a-3eb0-46be-8ba5-9979566daac6'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', 'filename': 'AdamW.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md': RefDocInfo(node_ids=['b74f605d-219a-408c-b7e8-cc36f2126221', '61036acc-861a-4e0b-b12c-431e8f3f3418'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md': RefDocInfo(node_ids=['df5f7349-8553-47a0-b3d8-4cfece7999c0', '597705e4-9982-4b05-8513-d2925adc7258', 'c8ca6285-796a-42c0-99fe-7dd9ddea04e8', 'b12069bf-0850-4ace-8c36-7885d32fa91a', 'c1be3add-c155-4eba-bd84-216f585c03f1', 'ceb5f3c1-51c7-4c94-80c7-8968c0e69914', '511d6839-bb37-4dc0-836a-761360e54a97'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md': RefDocInfo(node_ids=['e2ab8549-8b19-45bb-bb1f-9f7506864d4b'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/How Do Vision Transformers work?.md', 'filename': 'How Do Vision Transformers work?.md'}),\n",
       " '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md': RefDocInfo(node_ids=['52641d5c-49d6-4d46-81fd-5a98f1062cde', '4cc777ee-6146-4821-884d-20354df91ecd', '67ea54dc-b723-4fbc-ac7a-673ad7442345', '83294dac-b95b-418a-97c9-8b90c3a31dd3', '18ad8218-6e19-4628-b96d-fe8815e67e0e', '49b1b018-d51a-480d-8afa-e1e90dc503df', '480de224-690a-4e96-94a3-4359991c6634', '49bab0f1-2b39-4b9a-8ef7-14a30b94833a', '6c244109-849a-4f21-8402-5c886dacc4e5', '82ff0f52-d210-474a-ae55-3af8e4789e78', 'b2c143cd-25a8-4c78-b5ed-89ddc7bd3ac3', '6b7e44a8-2212-4cca-ac45-a0268f694954', 'ef4d3327-68db-468f-a946-ed4ea395b86e', '7197cbdb-fe2a-44b8-848b-f8c69083f334', '2625eaf3-5eef-4e61-9429-c1cfb99a58ee'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'})}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ref_doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(llama_index.core.indices.vector_store.base.VectorStoreIndex,\n",
       " llama_index.core.vector_stores.simple.SimpleVectorStore)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index), type(index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\"./data/tech_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.vector_stores import SimpleVectorStore\n",
    "from llama_index.core.storage.index_store import SimpleIndexStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"./data/tech_notes\"),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(\n",
    "        persist_dir=\"./data/tech_notes\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"./data/tech_notes\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.indices.vector_store.base.VectorStoreIndex"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file(index, file_path):\n",
    "        print(f\"Add file operations {file_path}\")\n",
    "        document = MarkdownReader().load_data(file_path)\n",
    "        for c in document:\n",
    "                if metadata_extractor:\n",
    "                        c.metadata = metadata_extractor(file_path)\n",
    "                c.doc_id = file_path\n",
    "                index.insert(c)\n",
    "        # index.insert(document[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add file operations /Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/C++.md\n"
     ]
    }
   ],
   "source": [
    "add_file(index, \"/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/C++.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='The context provided does not contain any information about C++ or its characteristics. Therefore, I cannot provide an answer to why C++ might be considered a funny language. If you have specific aspects or examples in mind, please share them for further discussion.', source_nodes=[NodeWithScore(node=TextNode(id_='b74f605d-219a-408c-b7e8-cc36f2126221', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, hash='5c60e7ff9f8d0482167e19c6833987cab89d54d780b24698df9b502ee730d28f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='61036acc-861a-4e0b-b12c-431e8f3f3418', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='52c1777ffba88e27ebcf0eb1dea3786d2bf1581f6476aed6ddd3e46f0c3807a8')}, text='- ^Character level language Modelling - The basic unit is a character ^a60c01\\n\\t- Therefore a much smaller vocabulary size\\n\\t- \\'The cat sat on the m\\' - the prediction would the character \\'a\\' to get to the word mat for this language model\\n\\t- Predicts on character level\\n\\t- Can handle out of vocabulary words and misspellings easily\\n- Word level language Modelling - The basic unit is a word\\n\\t- Much bigger vocabulary size\\n\\t- Output is much more coherent\\n\\n> Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively **interpolates between word level inputs for frequent symbol sequences and char- acter level inputs for infrequent symbol sequences.**\\n\\nClaude Explanation for Byte Pair\\n\\n\\n1. Character-level vs. Word-level Language Modeling:\\n\\nCharacter-level Language Modeling:\\n- Operates at the level of individual characters (letters, numbers, punctuation)\\n- Predicts the next character in a sequence\\n- Has a smaller vocabulary size (typically 26 letters, 10 digits, and some special characters)\\n- Can handle out-of-vocabulary words and misspellings more easily\\n\\nExample:\\nInput: \"The cat sat on the m\"\\nTask: Predict the next character (likely \"a\" to form \"mat\")\\n\\nWord-level Language Modeling:\\n- Operates at the level of whole words\\n- Predicts the next word in a sequence\\n- Has a larger vocabulary size (typically tens or hundreds of thousands of words)\\n- Generally produces more coherent output for larger contexts\\n\\nExample:\\nInput: \"The cat sat on the\"\\nTask: Predict the next word (e.g., \"mat\", \"chair\", \"roof\")\\n\\n2. Byte Pair Encoding (BPE):\\n\\nByte Pair Encoding, as explained in the GPT-2 paper \"Language Models are Unsupervised Multitask Learners,\" is a subword tokenization method that bridges the gap between character-level and word-level modeling.\\n\\nKey concepts:\\na) It starts with a vocabulary of individual characters and iteratively merges the most frequent pair of consecutive bytes or characters.\\nb) This process creates a vocabulary of subword units, which can represent both common words and parts of less common words.\\n\\nHow it works:\\n1. Initialize the vocabulary with individual characters.\\n2. Count the frequency of adjacent pairs of symbols in the training data.\\n3. Merge the most frequent pair and add it to the vocabulary.\\n4. Repeat steps 2-3 for a predetermined number of merges or until a desired vocabulary size is reached.\\n\\nExample:\\nLet\\'s say we have the following text: \"The dancer dances beautifully\"\\n\\nInitial vocabulary: [\\'T\\', \\'h\\', \\'e\\', \\' \\', \\'d\\', \\'a\\', \\'n\\', \\'c\\', \\'r\\', \\'s\\', \\'b\\', \\'u\\', \\'t\\', \\'i\\', \\'f\\', \\'l\\', \\'y\\']\\n\\nAfter some merges:\\n[\\'The\\', \\' \\', \\'d\\', \\'an\\', \\'c\\', \\'er\\', \\' \\', \\'d\\', \\'an\\', \\'ces\\', \\' \\', \\'b\\', \\'eau\\', \\'t\\', \\'i\\', \\'f\\', \\'ul\\', \\'ly\\']\\n\\nBenefits of BPE in the context of GPT-2:\\n1. Adaptability: It can handle both common and rare words effectively.\\n2. Efficiency: It creates a compact vocabulary that balances the trade-offs between character-level and word-level models.\\n3. Handling of unknown words: It can often break down unknown words into meaningful subword units.\\n\\nBy using BPE, GPT-2 achieves a balance between the flexibility of character-level models and the semantic coherence of word-level models. This allows it to work effectively across a wide range of languages and domains without requiring language-specific preprocessing or tokenization.\\n\\nWould you like me to elaborate on any specific aspect of these concepts?\\n\\n\\n\\n\\n\\n[[Metrics#Perplexity]]\\n\\nGPT 2 is trained on web text but evaluated on different datasets - children\\'s book test, lambada, etc without any fine tuning - Zero Shot Setting.\\n\\n\\nThe recommended way for creating train and test-splits for this dataset is to use \\n\\nZero shot performance could be used as baseline for performance for fine-tuning', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.2698353674790432), NodeWithScore(node=TextNode(id_='ceb5f3c1-51c7-4c94-80c7-8968c0e69914', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='943d08715248bfd9dac0c5a1e0635c76700c3fbfab5669651c4574b07803cdaa'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c1be3add-c155-4eba-bd84-216f585c03f1', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='fdbdf50fda29a4c01d673f15e8d0ac424cf0476460291fca46019c9f01bbf60c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='511d6839-bb37-4dc0-836a-761360e54a97', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b5986cd52c23350734ba0d2a6473d868e8593f3e81c9fca87bd1df92c1974d5d')}, text='Powers of 2\\n\\n- Everything in cuda works in powers of 2 and lots of kernels are written to keep in mind powers of 2.\\n- Calculations works well in chunks of powers of 2. Otherwise - it could be inefficient.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.23586583398566313)], metadata={'b74f605d-219a-408c-b7e8-cc36f2126221': {'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, 'ceb5f3c1-51c7-4c94-80c7-8968c0e69914': {'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"Why is c++ is a funny language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='f68eb60a-3eb0-46be-8ba5-9979566daac6', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', 'filename': 'AdamW.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/AdamW.md', 'filename': 'AdamW.md'}, hash='e728c471e04692930df982fd825732f4d5c05279cc7e2fd53a13256bab2b78e3')}, text='AdamW updates the weight decay part of adam\\n- Weight decay is a regularisation applied to large weights in the NN to move them towards zero\\n\\t- low weight decay could cause over-fitting\\n\\t- high weight decay could cause under-fitting', mimetype='text/plain', start_char_idx=1, end_char_idx=232, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docstore.get_document(\"f68eb60a-3eb0-46be-8ba5-9979566daac6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"The query about why C++ is considered a funny language isn't addressed in the provided context. The context focuses on language modeling, specifically character-level and word-level modeling, as well as Byte Pair Encoding in the context of GPT-2. If you have questions related to those topics or need clarification on them, feel free to ask!\", source_nodes=[NodeWithScore(node=TextNode(id_='b74f605d-219a-408c-b7e8-cc36f2126221', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, hash='5c60e7ff9f8d0482167e19c6833987cab89d54d780b24698df9b502ee730d28f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='61036acc-861a-4e0b-b12c-431e8f3f3418', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='52c1777ffba88e27ebcf0eb1dea3786d2bf1581f6476aed6ddd3e46f0c3807a8')}, text='- ^Character level language Modelling - The basic unit is a character ^a60c01\\n\\t- Therefore a much smaller vocabulary size\\n\\t- \\'The cat sat on the m\\' - the prediction would the character \\'a\\' to get to the word mat for this language model\\n\\t- Predicts on character level\\n\\t- Can handle out of vocabulary words and misspellings easily\\n- Word level language Modelling - The basic unit is a word\\n\\t- Much bigger vocabulary size\\n\\t- Output is much more coherent\\n\\n> Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively **interpolates between word level inputs for frequent symbol sequences and char- acter level inputs for infrequent symbol sequences.**\\n\\nClaude Explanation for Byte Pair\\n\\n\\n1. Character-level vs. Word-level Language Modeling:\\n\\nCharacter-level Language Modeling:\\n- Operates at the level of individual characters (letters, numbers, punctuation)\\n- Predicts the next character in a sequence\\n- Has a smaller vocabulary size (typically 26 letters, 10 digits, and some special characters)\\n- Can handle out-of-vocabulary words and misspellings more easily\\n\\nExample:\\nInput: \"The cat sat on the m\"\\nTask: Predict the next character (likely \"a\" to form \"mat\")\\n\\nWord-level Language Modeling:\\n- Operates at the level of whole words\\n- Predicts the next word in a sequence\\n- Has a larger vocabulary size (typically tens or hundreds of thousands of words)\\n- Generally produces more coherent output for larger contexts\\n\\nExample:\\nInput: \"The cat sat on the\"\\nTask: Predict the next word (e.g., \"mat\", \"chair\", \"roof\")\\n\\n2. Byte Pair Encoding (BPE):\\n\\nByte Pair Encoding, as explained in the GPT-2 paper \"Language Models are Unsupervised Multitask Learners,\" is a subword tokenization method that bridges the gap between character-level and word-level modeling.\\n\\nKey concepts:\\na) It starts with a vocabulary of individual characters and iteratively merges the most frequent pair of consecutive bytes or characters.\\nb) This process creates a vocabulary of subword units, which can represent both common words and parts of less common words.\\n\\nHow it works:\\n1. Initialize the vocabulary with individual characters.\\n2. Count the frequency of adjacent pairs of symbols in the training data.\\n3. Merge the most frequent pair and add it to the vocabulary.\\n4. Repeat steps 2-3 for a predetermined number of merges or until a desired vocabulary size is reached.\\n\\nExample:\\nLet\\'s say we have the following text: \"The dancer dances beautifully\"\\n\\nInitial vocabulary: [\\'T\\', \\'h\\', \\'e\\', \\' \\', \\'d\\', \\'a\\', \\'n\\', \\'c\\', \\'r\\', \\'s\\', \\'b\\', \\'u\\', \\'t\\', \\'i\\', \\'f\\', \\'l\\', \\'y\\']\\n\\nAfter some merges:\\n[\\'The\\', \\' \\', \\'d\\', \\'an\\', \\'c\\', \\'er\\', \\' \\', \\'d\\', \\'an\\', \\'ces\\', \\' \\', \\'b\\', \\'eau\\', \\'t\\', \\'i\\', \\'f\\', \\'ul\\', \\'ly\\']\\n\\nBenefits of BPE in the context of GPT-2:\\n1. Adaptability: It can handle both common and rare words effectively.\\n2. Efficiency: It creates a compact vocabulary that balances the trade-offs between character-level and word-level models.\\n3. Handling of unknown words: It can often break down unknown words into meaningful subword units.\\n\\nBy using BPE, GPT-2 achieves a balance between the flexibility of character-level models and the semantic coherence of word-level models. This allows it to work effectively across a wide range of languages and domains without requiring language-specific preprocessing or tokenization.\\n\\nWould you like me to elaborate on any specific aspect of these concepts?\\n\\n\\n\\n\\n\\n[[Metrics#Perplexity]]\\n\\nGPT 2 is trained on web text but evaluated on different datasets - children\\'s book test, lambada, etc without any fine tuning - Zero Shot Setting.\\n\\n\\nThe recommended way for creating train and test-splits for this dataset is to use \\n\\nZero shot performance could be used as baseline for performance for fine-tuning', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.2698353674790432), NodeWithScore(node=TextNode(id_='ceb5f3c1-51c7-4c94-80c7-8968c0e69914', embedding=None, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='943d08715248bfd9dac0c5a1e0635c76700c3fbfab5669651c4574b07803cdaa'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c1be3add-c155-4eba-bd84-216f585c03f1', node_type=<ObjectType.TEXT: '1'>, metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}, hash='fdbdf50fda29a4c01d673f15e8d0ac424cf0476460291fca46019c9f01bbf60c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='511d6839-bb37-4dc0-836a-761360e54a97', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b5986cd52c23350734ba0d2a6473d868e8593f3e81c9fca87bd1df92c1974d5d')}, text='Powers of 2\\n\\n- Everything in cuda works in powers of 2 and lots of kernels are written to keep in mind powers of 2.\\n- Calculations works well in chunks of powers of 2. Otherwise - it could be inefficient.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.23586583398566313)], metadata={'b74f605d-219a-408c-b7e8-cc36f2126221': {'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Language models are Unsupervised Multitask Learners - GPT-2.md', 'filename': 'Language models are Unsupervised Multitask Learners - GPT-2.md'}, 'ceb5f3c1-51c7-4c94-80c7-8968c0e69914': {'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/GPT2.md', 'filename': 'GPT2.md'}})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"Why is c++ is a funny language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docstore.ref_doc_exists(\"/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/C++.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_doc_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RefDocInfo(node_ids=['52641d5c-49d6-4d46-81fd-5a98f1062cde', '4cc777ee-6146-4821-884d-20354df91ecd', '67ea54dc-b723-4fbc-ac7a-673ad7442345', '83294dac-b95b-418a-97c9-8b90c3a31dd3', '18ad8218-6e19-4628-b96d-fe8815e67e0e', '49b1b018-d51a-480d-8afa-e1e90dc503df', '480de224-690a-4e96-94a3-4359991c6634', '49bab0f1-2b39-4b9a-8ef7-14a30b94833a', '6c244109-849a-4f21-8402-5c886dacc4e5', '82ff0f52-d210-474a-ae55-3af8e4789e78', 'b2c143cd-25a8-4c78-b5ed-89ddc7bd3ac3', '6b7e44a8-2212-4cca-ac45-a0268f694954', 'ef4d3327-68db-468f-a946-ed4ea395b86e', '7197cbdb-fe2a-44b8-848b-f8c69083f334', '2625eaf3-5eef-4e61-9429-c1cfb99a58ee'], metadata={'filepath': '/Users/mo/Library/Mobile Documents/iCloud~md~obsidian/Documents/MainVault/TechNotes/Deep Learning/Quantisation/A Visual Guide to Quantization.md', 'filename': 'A Visual Guide to Quantization.md'})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docstore.get_ref_doc_info(quant_doc_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorStoreIndex' object has no attribute 'get_doc_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_doc_id\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc3cfd5a7-17b4-4952-96aa-1ea8bcf4bda2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VectorStoreIndex' object has no attribute 'get_doc_id'"
     ]
    }
   ],
   "source": [
    "index.get_doc_id(\"c3cfd5a7-17b4-4952-96aa-1ea8bcf4bda2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "doc_id 52641d5c-49d6-4d46-81fd-5a98f1062cde not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_document\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m52641d5c-49d6-4d46-81fd-5a98f1062cde\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/obsidian-rag-chat/venv/lib/python3.12/site-packages/llama_index/core/storage/docstore/keyval_docstore.py:360\u001b[0m, in \u001b[0;36mKVDocumentStore.get_document\u001b[0;34m(self, doc_id, raise_error)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 360\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: doc_id 52641d5c-49d6-4d46-81fd-5a98f1062cde not found."
     ]
    }
   ],
   "source": [
    "index.docstore.get_document(\"52641d5c-49d6-4d46-81fd-5a98f1062cde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.delete_ref_doc(quant_doc_ref, delete_from_docstore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.docstore.get_ref_doc_info(quant_doc_ref)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
